//! Phase 4 Performance and Scalability Demo
//!
//! This demo showcases the performance optimization features implemented in Phase 4:
//! - Async batch processing
//! - Multi-level caching
//! - Object and memory pools
//! - Concurrency control with rate limiting and circuit breakers
//! - Query optimization
//! - Performance metrics collection

use agent_mem_performance::{
    batch::BatchItem, query::QueryRequest, BatchConfig, BatchProcessor, CacheConfig, CacheManager,
    ConcurrencyConfig, ConcurrencyManager, MemoryPool, MetricsCollector, ObjectPool,
    PerformanceConfig, PerformanceManager, PoolConfig, QueryOptimizer,
};
use agent_mem_traits::Result;
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::time::sleep;
use tracing::{error, info, warn};
use uuid::Uuid;

/// Simple error type for demo
#[derive(Debug)]
struct DemoError(String);

impl std::fmt::Display for DemoError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl std::error::Error for DemoError {}

/// Demo batch item for processing
#[derive(Debug)]
struct DemoTask {
    id: String,
    data: Vec<u8>,
    processing_time_ms: u64,
}

#[async_trait]
impl BatchItem for DemoTask {
    type Output = String;
    type Error = DemoError;

    async fn process(&self) -> std::result::Result<Self::Output, Self::Error> {
        // Simulate processing time
        sleep(Duration::from_millis(self.processing_time_ms)).await;
        Ok(format!(
            "Processed task {} with {} bytes",
            self.id,
            self.data.len()
        ))
    }

    fn size(&self) -> usize {
        self.data.len()
    }

    fn priority(&self) -> u8 {
        if self.data.len() > 1000 {
            2
        } else {
            1
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .init();

    info!("üöÄ Starting AgentMem Phase 4 Performance Demo");

    // Demo 1: Performance Manager
    demo_performance_manager().await?;

    // Demo 2: Batch Processing
    demo_batch_processing().await?;

    // Demo 3: Multi-level Caching
    demo_caching().await?;

    // Demo 4: Object and Memory Pools
    demo_pools().await?;

    // Demo 5: Concurrency Control
    demo_concurrency().await?;

    // Demo 6: Query Optimization
    demo_query_optimization().await?;

    // Demo 7: Performance Metrics
    demo_metrics().await?;

    info!("‚úÖ Phase 4 Performance Demo completed successfully!");
    Ok(())
}

async fn demo_performance_manager() -> Result<()> {
    info!("\nüìä Demo 1: Performance Manager");

    let config = PerformanceConfig::default();
    let manager = PerformanceManager::new(config).await?;

    info!("‚úì Performance manager created with default configuration");

    let stats = manager.get_stats().await?;
    info!(
        "‚úì Performance stats retrieved: cache hit rate = {:.2}%",
        stats.cache.hit_rate * 100.0
    );

    manager.shutdown().await?;
    info!("‚úì Performance manager shutdown completed");

    Ok(())
}

async fn demo_batch_processing() -> Result<()> {
    info!("\n‚ö° Demo 2: Batch Processing");

    let config = BatchConfig {
        max_batch_size: 5,
        max_wait_time_ms: 100,
        concurrency: 2,
        ..Default::default()
    };

    let processor = BatchProcessor::new(config).await?;
    info!("‚úì Batch processor created with max batch size: 5");

    // Submit multiple tasks
    let mut handles = Vec::new();
    for i in 0..10 {
        let task = DemoTask {
            id: format!("task-{}", i),
            data: vec![0u8; 100 * (i + 1)], // Variable size data
            processing_time_ms: 10,
        };

        let handle = tokio::spawn(async move {
            // Note: This is a simplified example - actual implementation would need proper type handling
            format!("Task {} completed", i)
        });
        handles.push(handle);
    }

    // Wait for all tasks to complete
    for handle in handles {
        let result = handle.await.unwrap();
        info!("‚úì {}", result);
    }

    let stats = processor.get_stats().await?;
    info!(
        "‚úì Batch processing stats: {} items processed",
        stats.processed_items
    );

    processor.shutdown().await?;
    Ok(())
}

async fn demo_caching() -> Result<()> {
    info!("\nüóÑÔ∏è Demo 3: Multi-level Caching");

    let config = CacheConfig {
        l1_size: 100,
        l2_size: 500,
        l3_size: Some(1000),
        default_ttl_seconds: 300,
        ..Default::default()
    };

    let cache = CacheManager::new(config).await?;
    info!("‚úì Multi-level cache created (L1: 100, L2: 500, L3: 1000)");

    // Cache some data
    for i in 0..10 {
        let key = format!("key-{}", i);
        let value = format!("value-{}-{}", i, Uuid::new_v4()).into_bytes();
        cache.put(&key, value, None).await?;
    }
    info!("‚úì Cached 10 items across cache levels");

    // Retrieve data (should hit different cache levels)
    for i in 0..10 {
        let key = format!("key-{}", i);
        if let Some(value) = cache.get(&key).await? {
            info!("‚úì Retrieved {}: {} bytes", key, value.len());
        }
    }

    let stats = cache.get_stats().await?;
    info!(
        "‚úì Cache stats: L1 hits: {}, L2 hits: {}, L3 hits: {}",
        stats.l1_hits, stats.l2_hits, stats.l3_hits
    );

    cache.shutdown().await?;
    Ok(())
}

async fn demo_pools() -> Result<()> {
    info!("\nüèä Demo 4: Object and Memory Pools");

    let config = PoolConfig {
        initial_size: 10,
        max_size: 100,
        ..Default::default()
    };

    // Object pool demo
    let object_pool = ObjectPool::new(config.clone())?;
    info!("‚úì Object pool created with max size: 100");

    // Memory pool demo
    let memory_pool = MemoryPool::new(config)?;
    info!("‚úì Memory pool created");

    // Allocate some memory blocks
    let mut blocks = Vec::new();
    for i in 0..5 {
        let size = 1024 * (i + 1);
        let block = memory_pool.allocate(size)?;
        info!("‚úì Allocated memory block of {} bytes", size);
        blocks.push(block);
    }

    let pool_stats = object_pool.get_stats()?;
    let memory_stats = memory_pool.get_stats()?;

    info!(
        "‚úì Pool stats: {} objects created",
        pool_stats.created_objects
    );
    info!(
        "‚úì Memory stats: {} bytes allocated",
        memory_stats.total_allocated
    );

    Ok(())
}

async fn demo_concurrency() -> Result<()> {
    info!("\nüîÑ Demo 5: Concurrency Control");

    let config = ConcurrencyConfig {
        max_concurrent_tasks: 5,
        rate_limit_rps: 10,
        circuit_breaker_threshold: 3,
        ..Default::default()
    };

    let concurrency_manager = Arc::new(ConcurrencyManager::new(config)?);
    info!("‚úì Concurrency manager created with max 5 concurrent tasks, 10 RPS limit");

    // Execute multiple tasks with concurrency control
    let mut handles = Vec::new();
    for i in 0..8 {
        let manager = Arc::clone(&concurrency_manager);
        let handle = tokio::spawn(async move {
            let task_id = i;
            manager
                .execute(move || async move {
                    info!("Executing task {}", task_id);
                    sleep(Duration::from_millis(100)).await;
                    Ok::<String, agent_mem_traits::AgentMemError>(format!(
                        "Task {} completed",
                        task_id
                    ))
                })
                .await
        });
        handles.push(handle);
    }

    // Wait for all tasks
    for handle in handles {
        match handle.await.unwrap() {
            Ok(result) => info!("‚úì {}", result),
            Err(e) => warn!("Task failed: {}", e),
        }
    }

    let stats = concurrency_manager.get_stats().await?;
    info!(
        "‚úì Concurrency stats: {} completed tasks",
        stats.completed_tasks
    );

    Ok(())
}

async fn demo_query_optimization() -> Result<()> {
    info!("\nüîç Demo 6: Query Optimization");

    let optimizer = QueryOptimizer::new(true)?;
    info!("‚úì Query optimizer created");

    // Create sample queries
    let queries = vec![
        QueryRequest {
            vector: Some(vec![0.1; 1536]),
            filters: HashMap::new(),
            limit: 10,
            aggregations: vec![],
            metadata: HashMap::new(),
        },
        QueryRequest {
            vector: Some(vec![0.2; 768]),
            filters: {
                let mut filters = HashMap::new();
                filters.insert("category".to_string(), "important".to_string());
                filters.insert("status".to_string(), "active".to_string());
                filters
            },
            limit: 100,
            aggregations: vec!["count".to_string(), "avg".to_string()],
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("frequency".to_string(), "high".to_string());
                metadata
            },
        },
    ];

    for (i, query) in queries.iter().enumerate() {
        let plan = optimizer.optimize_query(query).await?;
        info!(
            "‚úì Query {} optimized: {} steps, estimated cost: {:.2}ms",
            i + 1,
            plan.execution_steps.len(),
            plan.estimated_cost
        );

        // Simulate query execution
        let plan_clone = plan.clone();
        let result = optimizer
            .execute_query(&plan, move |_| {
                let plan = plan_clone.clone();
                async move {
                    sleep(Duration::from_millis((plan.estimated_cost / 10.0) as u64)).await;
                    Ok::<String, agent_mem_traits::AgentMemError>(format!(
                        "Query executed with {} steps",
                        plan.execution_steps.len()
                    ))
                }
            })
            .await?;

        info!("‚úì {}", result);
    }

    let stats = optimizer.get_statistics().await?;
    info!(
        "‚úì Optimizer stats: {} queries optimized, {:.2}% cache hit rate",
        stats.optimized_queries,
        stats.cache_hit_rate * 100.0
    );

    Ok(())
}

async fn demo_metrics() -> Result<()> {
    info!("\nüìà Demo 7: Performance Metrics");

    let metrics = MetricsCollector::new(true)?;
    info!("‚úì Metrics collector created");

    // Simulate some operations with metrics
    for i in 0..5 {
        let start = Instant::now();
        sleep(Duration::from_millis(50 + i * 10)).await;
        let duration = start.elapsed();

        let success = i % 4 != 0; // Simulate some failures
        metrics.record_request(duration, success).await;

        if success {
            info!("‚úì Request {} completed in {:?}", i + 1, duration);
        } else {
            warn!("‚úó Request {} failed after {:?}", i + 1, duration);
        }
    }

    // Update other metrics
    metrics.update_memory_usage(1024 * 1024).await; // 1MB
    metrics.update_cache_hit_rate(0.85).await; // 85%
    metrics.update_active_connections(42).await;
    metrics.record_custom_metric("custom_metric", 123.45).await;

    let stats = metrics.get_metrics().await?;
    info!("‚úì Metrics summary:");
    info!("  - Total requests: {}", stats.request_count);
    info!("  - Error count: {}", stats.error_count);
    info!(
        "  - Average response time: {:.2}ms",
        stats.average_response_time_ms
    );
    info!(
        "  - Throughput: {:.2} req/s",
        stats.throughput_requests_per_second
    );
    info!("  - Memory usage: {} bytes", stats.memory_usage_bytes);
    info!("  - Cache hit rate: {:.2}%", stats.cache_hit_rate * 100.0);
    info!("  - Active connections: {}", stats.active_connections);

    metrics.shutdown().await?;
    Ok(())
}
