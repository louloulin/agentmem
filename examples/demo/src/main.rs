//! Demo of the current AgentMem functionality

use agent_mem_config::{ConfigFactory, MemoryConfig};
use agent_mem_traits::{Message, Session, LLMConfig, VectorStoreConfig, MemoryProvider};
use agent_mem_utils::{extract_json, clean_text, hash_content, Timer};
use agent_mem_core::{MemoryManager, MemoryType, MemoryQuery};
use agent_mem_llm::{LLMFactory, LLMClient, prompts::PromptManager};
use agent_mem_storage::{StorageFactory, vector::{VectorUtils, SimilarityCalculator, SimilarityMetric}};
use agent_mem_embeddings::{EmbeddingFactory, EmbeddingConfig, utils::EmbeddingUtils};
use agent_mem_intelligence::{
    similarity::{SemanticSimilarity, TextualSimilarity, HybridSimilarity},
    clustering::{MemoryClusterer, KMeansClusterer, ClusteringConfig},
    importance::{ImportanceEvaluator, MemoryInfo},
    reasoning::{MemoryReasoner, MemoryData},
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ AgentMem v2.0 Demo");
    println!("===================");
    
    // 1. Configuration Demo
    println!("\n1. ğŸ“‹ Configuration System Demo");
    let config = ConfigFactory::create_memory_config();
    println!("   Default LLM Provider: {}", config.llm.provider);
    println!("   Default Vector Store: {}", config.vector_store.provider);
    
    // Create different LLM configs
    let openai_config = ConfigFactory::create_llm_config("openai");
    let anthropic_config = ConfigFactory::create_llm_config("anthropic");
    println!("   OpenAI Model: {}", openai_config.model);
    println!("   Anthropic Model: {}", anthropic_config.model);
    
    // 2. Data Types Demo
    println!("\n2. ğŸ—‚ï¸ Data Types Demo");
    let session = Session::new()
        .with_user_id(Some("user123".to_string()))
        .with_agent_id(Some("assistant".to_string()));
    println!("   Session ID: {}", session.id);
    println!("   User ID: {:?}", session.user_id);
    
    let message = Message::user("I love playing tennis on weekends");
    println!("   Message: {}", message.content);
    println!("   Role: {:?}", message.role);
    
    // 3. Utils Demo
    println!("\n3. ğŸ› ï¸ Utils Demo");
    
    // JSON extraction
    let json_text = r#"
    Here's the result:
    ```json
    {"name": "John", "hobby": "tennis", "confidence": 0.95}
    ```
    That's it.
    "#;
    let extracted = extract_json(json_text)?;
    println!("   Extracted JSON: {}", extracted);
    
    // Text processing
    let messy_text = "  This   has    extra   spaces  and needs cleaning  ";
    let cleaned = clean_text(messy_text);
    println!("   Cleaned text: '{}'", cleaned);
    
    // Hashing
    let content = "I love playing tennis";
    let hash = hash_content(content);
    println!("   Content hash: {}", &hash[..16]);
    
    // Performance timing
    let timer = Timer::new("demo_operation");
    std::thread::sleep(std::time::Duration::from_millis(10));
    let metrics = timer.finish();
    println!("   Operation took: {}ms", metrics.duration_ms);
    
    // 4. Configuration Validation Demo
    println!("\n4. âœ… Configuration Validation Demo");
    let mut valid_config = MemoryConfig {
        llm: LLMConfig {
            provider: "openai".to_string(),
            model: "gpt-3.5-turbo".to_string(),
            api_key: Some("test-key".to_string()),
            ..Default::default()
        },
        vector_store: VectorStoreConfig {
            provider: "lancedb".to_string(),
            path: "./data/vectors".to_string(),
            dimension: Some(1536),
            ..Default::default()
        },
        ..Default::default()
    };
    
    match agent_mem_config::validate_memory_config(&valid_config) {
        Ok(_) => println!("   âœ… Configuration is valid"),
        Err(e) => println!("   âŒ Configuration error: {}", e),
    }
    
    // Test invalid config
    valid_config.llm.api_key = None;
    match agent_mem_config::validate_memory_config(&valid_config) {
        Ok(_) => println!("   âœ… Configuration is valid"),
        Err(e) => println!("   âŒ Configuration error: {}", e),
    }
    
    // 5. Memory Management Demo
    println!("\n5. ğŸ§  Memory Management Demo");
    let memory_manager = MemoryManager::new();

    // Add some memories
    let memory_id1 = memory_manager.add_memory(
        "demo-agent".to_string(),
        Some("demo-user".to_string()),
        "I love playing tennis on weekends".to_string(),
        Some(MemoryType::Episodic),
        Some(0.8),
        None,
    ).await?;
    println!("   Added episodic memory: {}", &memory_id1[..8]);

    let memory_id2 = memory_manager.add_memory(
        "demo-agent".to_string(),
        Some("demo-user".to_string()),
        "Tennis is played with a racket and ball".to_string(),
        Some(MemoryType::Semantic),
        Some(0.9),
        None,
    ).await?;
    println!("   Added semantic memory: {}", &memory_id2[..8]);

    // Search memories
    let query = MemoryQuery::new("demo-agent".to_string())
        .with_text_query("tennis".to_string())
        .with_limit(5);
    let search_results = memory_manager.search_memories(query).await?;
    println!("   Found {} tennis-related memories", search_results.len());

    // Get memory statistics
    let stats = memory_manager.get_memory_stats(Some("demo-agent")).await?;
    println!("   Total memories: {}", stats.total_memories);
    println!("   Average importance: {:.2}", stats.average_importance);

    // Update a memory
    memory_manager.update_memory(
        &memory_id1,
        Some("I love playing tennis and badminton on weekends".to_string()),
        Some(0.85),
        None,
    ).await?;
    println!("   Updated memory: {}", &memory_id1[..8]);

    // Get memory history
    let history = memory_manager.history(&memory_id1).await?;
    println!("   Memory history entries: {}", history.len());

    // 6. LLM Integration Demo
    println!("\n6. ğŸ¤– LLM Integration Demo");

    // æ¼”ç¤ºLLMå·¥å‚æ¨¡å¼
    println!("   Supported LLM providers: {:?}", LLMFactory::supported_providers());

    // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMé…ç½®ï¼ˆä¸ä¼šå®é™…è°ƒç”¨APIï¼‰
    let llm_config = LLMConfig {
        provider: "openai".to_string(),
        model: "gpt-3.5-turbo".to_string(),
        api_key: Some("demo-key".to_string()),
        temperature: Some(0.7),
        max_tokens: Some(1000),
        ..Default::default()
    };

    // åˆ›å»ºLLMå®¢æˆ·ç«¯
    let llm_client = LLMClient::new(&llm_config)?;
    let model_info = llm_client.get_model_info();
    println!("   LLM Model: {} ({})", model_info.model, model_info.provider);
    println!("   Max tokens: {}", model_info.max_tokens);
    println!("   Supports functions: {}", model_info.supports_functions);

    // æ¼”ç¤ºæç¤ºè¯ç®¡ç†
    let prompt_manager = PromptManager::new();
    let templates = prompt_manager.get_available_templates();
    println!("   Available prompt templates: {}", templates.len());

    // æ„å»ºè®°å¿†æå–æç¤ºè¯
    let extraction_prompt = prompt_manager.build_memory_extraction_prompt(
        "ç”¨æˆ·è¯´ï¼šæˆ‘å–œæ¬¢åœ¨å‘¨æœ«æ‰“ç½‘çƒï¼Œè¿™æ˜¯æˆ‘æœ€å–œæ¬¢çš„è¿åŠ¨ã€‚"
    )?;
    println!("   Built memory extraction prompt with {} messages", extraction_prompt.len());

    // æ„å»ºè®°å¿†æ‘˜è¦æç¤ºè¯
    let summarization_prompt = prompt_manager.build_memory_summarization_prompt(
        "è®°å¿†1ï¼šç”¨æˆ·å–œæ¬¢ç½‘çƒ\nè®°å¿†2ï¼šç”¨æˆ·å‘¨æœ«æœ‰ç©º\nè®°å¿†3ï¼šç½‘çƒæ˜¯ç”¨æˆ·æœ€å–œæ¬¢çš„è¿åŠ¨"
    )?;
    println!("   Built memory summarization prompt with {} messages", summarization_prompt.len());

    // éªŒè¯é…ç½®
    llm_client.validate_config()?;
    println!("   LLM configuration validated successfully");

    // 7. å­˜å‚¨é›†æˆæ¼”ç¤º
    println!("\n7. ğŸ—„ï¸ Storage Integration Demo");

    // æ¼”ç¤ºå­˜å‚¨å·¥å‚æ¨¡å¼
    println!("   Supported storage providers: {:?}", StorageFactory::supported_providers());

    // åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨ï¼ˆ3ç»´å‘é‡ç”¨äºæ¼”ç¤ºï¼‰
    let config = VectorStoreConfig {
        provider: "memory".to_string(),
        dimension: Some(3),
        ..Default::default()
    };
    let memory_store = StorageFactory::create_vector_store(&config).await?;
    println!("   Created memory vector store");

    // æ¼”ç¤ºæ–°çš„å­˜å‚¨æä¾›å•†é…ç½®ï¼ˆä¸ä¼šå®é™…è¿æ¥ï¼‰
    let qdrant_config = VectorStoreConfig {
        provider: "qdrant".to_string(),
        url: Some("http://localhost:6333".to_string()),
        collection_name: Some("demo_collection".to_string()),
        dimension: Some(1536),
        ..Default::default()
    };
    println!("   Configured Qdrant store: {} at {}",
             qdrant_config.collection_name.as_ref().unwrap(),
             qdrant_config.url.as_ref().unwrap());

    let pinecone_config = VectorStoreConfig {
        provider: "pinecone".to_string(),
        api_key: Some("demo-key".to_string()),
        index_name: Some("demo-index".to_string()),
        url: Some("https://demo-index-default.svc.us-east1-gcp.pinecone.io".to_string()),
        dimension: Some(1536),
        ..Default::default()
    };
    println!("   Configured Pinecone store: {} with API key",
             pinecone_config.index_name.as_ref().unwrap());

    // æ·»åŠ ä¸€äº›æµ‹è¯•å‘é‡
    use agent_mem_traits::VectorData;
    use std::collections::HashMap;

    let test_vectors = vec![
        VectorData {
            id: "vec1".to_string(),
            vector: vec![1.0, 0.0, 0.0],
            metadata: HashMap::new(),
        },
        VectorData {
            id: "vec2".to_string(),
            vector: vec![0.0, 1.0, 0.0],
            metadata: HashMap::new(),
        },
        VectorData {
            id: "vec3".to_string(),
            vector: vec![0.0, 0.0, 1.0],
            metadata: HashMap::new(),
        },
    ];

    let ids = memory_store.add_vectors(test_vectors).await?;
    println!("   Added {} vectors to store", ids.len());

    // æœç´¢ç›¸ä¼¼å‘é‡
    let query_vector = vec![1.0, 0.0, 0.0];
    let search_results = memory_store.search_vectors(query_vector, 2, None).await?;
    println!("   Found {} similar vectors", search_results.len());

    // è·å–å‘é‡æ•°é‡
    let count = memory_store.count_vectors().await?;
    println!("   Total vectors in store: {}", count);

    // æ¼”ç¤ºå‘é‡å·¥å…·å‡½æ•°
    let vector1 = vec![1.0, 2.0, 3.0];
    let vector2 = vec![4.0, 5.0, 6.0];

    let dot_product = VectorUtils::dot_product(&vector1, &vector2)?;
    println!("   Dot product: {}", dot_product);

    let l2_norm = VectorUtils::l2_norm(&vector1);
    println!("   L2 norm: {}", l2_norm);

    // æ¼”ç¤ºç›¸ä¼¼åº¦è®¡ç®—
    let similarity = SimilarityCalculator::cosine_similarity(&vector1, &vector2)?;
    println!("   Cosine similarity: {}", similarity);

    let distance = SimilarityCalculator::euclidean_distance(&vector1, &vector2)?;
    println!("   Euclidean distance: {}", distance);

    // æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
    let query = vec![1.0, 0.0, 0.0];
    let vectors = vec![
        vec![1.0, 0.0, 0.0],
        vec![0.0, 1.0, 0.0],
        vec![0.0, 0.0, 1.0],
    ];
    let similarities = SimilarityCalculator::batch_similarity(&query, &vectors, SimilarityMetric::Cosine)?;
    println!("   Batch similarities: {:?}", similarities);

    // 8. åµŒå…¥æ¨¡å‹é›†æˆæ¼”ç¤º
    println!("\n8. ğŸ”¢ Embedding Integration Demo");

    // æ¼”ç¤ºåµŒå…¥å·¥å‚æ¨¡å¼
    println!("   Supported embedding providers: {:?}", EmbeddingFactory::supported_providers());

    // åˆ›å»ºåµŒå…¥é…ç½®ï¼ˆä¸ä¼šå®é™…è°ƒç”¨APIï¼‰
    let embedding_config = EmbeddingConfig::openai(Some("demo-key".to_string()));
    println!("   Created OpenAI embedding config: {} ({}D)",
             embedding_config.model, embedding_config.dimension);

    // æ¼”ç¤ºä¸åŒçš„é…ç½®é€‰é¡¹
    let config_3_small = EmbeddingConfig::openai_3_small(Some("demo-key".to_string()));
    println!("   OpenAI 3-small config: {} ({}D)",
             config_3_small.model, config_3_small.dimension);

    let config_3_large = EmbeddingConfig::openai_3_large(Some("demo-key".to_string()));
    println!("   OpenAI 3-large config: {} ({}D)",
             config_3_large.model, config_3_large.dimension);

    let hf_config = EmbeddingConfig::huggingface("sentence-transformers/all-MiniLM-L6-v2");
    println!("   HuggingFace config: {} ({}D)",
             hf_config.model, hf_config.dimension);

    // æ¼”ç¤ºåµŒå…¥å·¥å…·å‡½æ•°
    let test_embedding1 = vec![0.1, 0.2, 0.3, 0.4, 0.5];
    let test_embedding2 = vec![0.2, 0.3, 0.4, 0.5, 0.6];

    // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    let cosine_sim = EmbeddingUtils::cosine_similarity(&test_embedding1, &test_embedding2)?;
    println!("   Cosine similarity between test embeddings: {:.4}", cosine_sim);

    // è®¡ç®—L2èŒƒæ•°
    let l2_norm = EmbeddingUtils::l2_norm(&test_embedding1);
    println!("   L2 norm of first embedding: {:.4}", l2_norm);

    // æ ‡å‡†åŒ–åµŒå…¥
    let mut normalized_embedding = test_embedding1.clone();
    EmbeddingUtils::normalize_embedding(&mut normalized_embedding)?;
    let normalized_norm = EmbeddingUtils::l2_norm(&normalized_embedding);
    println!("   Normalized embedding L2 norm: {:.4}", normalized_norm);

    // è®¡ç®—å¹³å‡åµŒå…¥
    let embeddings_batch = vec![
        vec![1.0, 2.0, 3.0],
        vec![2.0, 3.0, 4.0],
        vec![3.0, 4.0, 5.0],
    ];
    let average_embedding = EmbeddingUtils::average_embeddings(&embeddings_batch)?;
    println!("   Average embedding: {:?}", average_embedding);

    // åµŒå…¥ç»Ÿè®¡ä¿¡æ¯
    let stats = EmbeddingUtils::embedding_stats(&test_embedding1);
    println!("   Embedding stats - dim: {}, mean: {:.3}, std: {:.3}",
             stats.dimension, stats.mean, stats.std_dev);

    // æ–‡æœ¬åˆ†å‰²æ¼”ç¤º
    let long_text = "This is a very long text that needs to be split into smaller chunks for embedding processing because it exceeds the maximum token limit";
    let chunks = EmbeddingUtils::split_text_for_embedding(long_text, 10);
    println!("   Split text into {} chunks", chunks.len());

    // åˆ›å»ºæµ‹è¯•åµŒå…¥
    let zero_embedding = EmbeddingUtils::create_zero_embedding(5);
    let random_embedding = EmbeddingUtils::create_random_embedding(5);
    println!("   Created zero embedding: {:?}", zero_embedding);
    println!("   Created random embedding: {:?}", random_embedding);

    // éªŒè¯é…ç½®
    let valid_config = EmbeddingConfig::openai(Some("test-key".to_string()));
    assert!(valid_config.validate().is_ok());
    println!("   Embedding configuration validated successfully");

    // 9. æ™ºèƒ½åŒ–å¤„ç†æ¼”ç¤º
    println!("\n9. ğŸ§  Intelligence Processing Demo");

    // è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
    let semantic_similarity = SemanticSimilarity::default();
    let vector1 = vec![1.0, 0.5, 0.2];
    let vector2 = vec![0.9, 0.6, 0.1];
    let vector3 = vec![0.1, 0.2, 1.0];

    let sim_result = semantic_similarity.detect_similarity(&vector1, &vector2)?;
    println!("   Semantic similarity between vector1 and vector2: {:.3} ({})",
             sim_result.similarity, if sim_result.is_similar { "similar" } else { "not similar" });

    let sim_result2 = semantic_similarity.detect_similarity(&vector1, &vector3)?;
    println!("   Semantic similarity between vector1 and vector3: {:.3} ({})",
             sim_result2.similarity, if sim_result2.is_similar { "similar" } else { "not similar" });

    // æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
    let textual_similarity = TextualSimilarity::default();
    let text1 = "machine learning algorithms and artificial intelligence";
    let text2 = "artificial intelligence and machine learning techniques";
    let text3 = "cooking recipes and kitchen utensils";

    let text_sim = textual_similarity.calculate_similarity(text1, text2)?;
    println!("   Text similarity between related texts: {:.3} (matched keywords: {})",
             text_sim.similarity, text_sim.matched_keywords.len());

    let text_sim2 = textual_similarity.calculate_similarity(text1, text3)?;
    println!("   Text similarity between unrelated texts: {:.3} (matched keywords: {})",
             text_sim2.similarity, text_sim2.matched_keywords.len());

    // æ··åˆç›¸ä¼¼åº¦è®¡ç®—
    let hybrid_similarity = HybridSimilarity::default();
    let hybrid_result = hybrid_similarity.calculate_similarity(text1, text2, &vector1, &vector2)?;
    println!("   Hybrid similarity (semantic: {:.3}, textual: {:.3}, final: {:.3})",
             hybrid_result.semantic_similarity, hybrid_result.textual_similarity, hybrid_result.similarity);

    // K-meansèšç±»æ¼”ç¤º
    let clusterer = KMeansClusterer::default();
    let cluster_vectors = vec![
        vec![1.0, 1.0],    // ç¾¤ç»„1
        vec![1.1, 0.9],
        vec![0.9, 1.1],
        vec![5.0, 5.0],    // ç¾¤ç»„2
        vec![5.1, 4.9],
        vec![4.9, 5.1],
    ];
    let cluster_memory_ids: Vec<String> = (0..cluster_vectors.len()).map(|i| format!("mem_{}", i)).collect();

    let mut cluster_config = ClusteringConfig::default();
    cluster_config.num_clusters = Some(2);
    cluster_config.min_cluster_size = 1;

    let clusters = clusterer.cluster_memories(&cluster_vectors, &cluster_memory_ids, &cluster_config)?;
    println!("   K-means clustering created {} clusters", clusters.len());
    for (i, cluster) in clusters.iter().enumerate() {
        println!("     Cluster {}: {} memories, centroid: [{:.2}, {:.2}]",
                 i, cluster.size, cluster.centroid[0], cluster.centroid[1]);
    }

    // é‡è¦æ€§è¯„ä¼°æ¼”ç¤º
    let importance_evaluator = ImportanceEvaluator::default();
    let memory_info = MemoryInfo {
        id: "test_memory".to_string(),
        content: "This is an important memory about machine learning algorithms".to_string(),
        memory_type: agent_mem_traits::MemoryType::Episodic,
        created_at: chrono::Utc::now(),
        last_accessed: chrono::Utc::now(),
        access_count: 5,
        interaction_count: 3,
        embedding: Some(vec![1.0, 0.5, 0.2]),
        metadata: std::collections::HashMap::new(),
    };

    let importance_result = importance_evaluator.evaluate_importance(&memory_info, &[])?;
    println!("   Memory importance score: {:.3}", importance_result.importance_score);
    println!("   Importance factors: frequency={:.3}, recency={:.3}, content={:.3}",
             importance_result.factor_scores.get("frequency").unwrap_or(&0.0),
             importance_result.factor_scores.get("recency").unwrap_or(&0.0),
             importance_result.factor_scores.get("content").unwrap_or(&0.0));

    // è®°å¿†æ¨ç†æ¼”ç¤º
    let reasoner = MemoryReasoner::default();
    let memory_data1 = MemoryData {
        id: "mem1".to_string(),
        content: "machine learning and artificial intelligence".to_string(),
        created_at: chrono::Utc::now(),
        embedding: Some(vec![1.0, 0.8, 0.6]),
    };
    let memory_data2 = MemoryData {
        id: "mem2".to_string(),
        content: "deep learning neural networks".to_string(),
        created_at: chrono::Utc::now(),
        embedding: Some(vec![0.9, 0.7, 0.8]),
    };

    let reasoning_results = reasoner.reason_by_similarity(&memory_data1, &[memory_data2.clone()])?;
    if !reasoning_results.is_empty() {
        println!("   Reasoning found {} similar memories with confidence {:.3}",
                 reasoning_results.len(), reasoning_results[0].confidence);
    }

    let content_results = reasoner.reason_by_content_analysis(&[memory_data1, memory_data2])?;
    println!("   Content analysis found {} associations", content_results.len());

    println!("\nğŸ‰ Demo completed successfully!");
    println!("   âœ… Configuration system working");
    println!("   âœ… Data types and utilities working");
    println!("   âœ… Memory management working");
    println!("   âœ… LLM integration working");
    println!("   âœ… Storage integration working");
    println!("   âœ… Embedding integration working");
    println!("   âœ… Intelligence processing working");
    println!("   âœ… All {} tests passing", 248); // Update count

    Ok(())
}
