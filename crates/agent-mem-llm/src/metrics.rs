//! LLM æ€§èƒ½ç›‘æ§æ¨¡å—
//!
//! æä¾› LLM è°ƒç”¨çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æï¼ŒåŒ…æ‹¬ï¼š
//! - è¯·æ±‚å»¶è¿Ÿè¿½è¸ª
//! - Token ä½¿ç”¨ç»Ÿè®¡
//! - é”™è¯¯ç‡ç»Ÿè®¡
//! - æˆæœ¬è¿½è¸ª

use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tracing::{debug, info};

/// LLM è°ƒç”¨æŒ‡æ ‡
#[derive(Debug, Clone)]
pub struct LLMMetrics {
    /// æä¾›å•†åç§°
    pub provider: String,
    /// æ¨¡å‹åç§°
    pub model: String,
    /// è¯·æ±‚å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub latency_ms: u64,
    /// è¾“å…¥ token æ•°é‡
    pub input_tokens: u32,
    /// è¾“å‡º token æ•°é‡
    pub output_tokens: u32,
    /// æ€» token æ•°é‡
    pub total_tokens: u32,
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// é”™è¯¯ç±»å‹ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    pub error_type: Option<String>,
    /// æ—¶é—´æˆ³
    pub timestamp: Instant,
}

impl LLMMetrics {
    /// åˆ›å»ºæ–°çš„æŒ‡æ ‡
    pub fn new(provider: String, model: String) -> Self {
        Self {
            provider,
            model,
            latency_ms: 0,
            input_tokens: 0,
            output_tokens: 0,
            total_tokens: 0,
            success: false,
            error_type: None,
            timestamp: Instant::now(),
        }
    }

    /// è®¾ç½®å»¶è¿Ÿ
    pub fn with_latency(mut self, latency: Duration) -> Self {
        self.latency_ms = latency.as_millis() as u64;
        self
    }

    /// è®¾ç½® token æ•°é‡
    pub fn with_tokens(mut self, input: u32, output: u32) -> Self {
        self.input_tokens = input;
        self.output_tokens = output;
        self.total_tokens = input + output;
        self
    }

    /// è®¾ç½®æˆåŠŸçŠ¶æ€
    pub fn with_success(mut self, success: bool) -> Self {
        self.success = success;
        self
    }

    /// è®¾ç½®é”™è¯¯ç±»å‹
    pub fn with_error(mut self, error_type: String) -> Self {
        self.error_type = Some(error_type);
        self.success = false;
        self
    }

    /// ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    pub fn estimated_cost(&self) -> f64 {
        // ç®€åŒ–çš„æˆæœ¬ä¼°ç®—ï¼ˆå®é™…æˆæœ¬å› æä¾›å•†å’Œæ¨¡å‹è€Œå¼‚ï¼‰
        let input_cost_per_1k = match self.provider.as_str() {
            "openai" => match self.model.as_str() {
                m if m.contains("gpt-4") => 0.03,
                m if m.contains("gpt-3.5") => 0.0015,
                _ => 0.002,
            },
            "anthropic" => 0.008,
            "azure" => 0.002,
            _ => 0.001,
        };

        let output_cost_per_1k = input_cost_per_1k * 2.0; // è¾“å‡ºé€šå¸¸æ˜¯è¾“å…¥çš„2å€

        let input_cost = (self.input_tokens as f64 / 1000.0) * input_cost_per_1k;
        let output_cost = (self.output_tokens as f64 / 1000.0) * output_cost_per_1k;

        input_cost + output_cost
    }
}

/// LLM æ€§èƒ½ç»Ÿè®¡
#[derive(Debug, Clone)]
pub struct LLMStats {
    /// æ€»è¯·æ±‚æ•°
    pub total_requests: u64,
    /// æˆåŠŸè¯·æ±‚æ•°
    pub successful_requests: u64,
    /// å¤±è´¥è¯·æ±‚æ•°
    pub failed_requests: u64,
    /// å¹³å‡å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub avg_latency_ms: f64,
    /// æœ€å°å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub min_latency_ms: u64,
    /// æœ€å¤§å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub max_latency_ms: u64,
    /// P50 å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub p50_latency_ms: u64,
    /// P95 å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub p95_latency_ms: u64,
    /// P99 å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub p99_latency_ms: u64,
    /// æ€»è¾“å…¥ token æ•°
    pub total_input_tokens: u64,
    /// æ€»è¾“å‡º token æ•°
    pub total_output_tokens: u64,
    /// æ€» token æ•°
    pub total_tokens: u64,
    /// æ€»æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    pub total_cost: f64,
    /// é”™è¯¯ç‡
    pub error_rate: f64,
    /// æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„çš„é”™è¯¯æ•°
    pub errors_by_type: HashMap<String, u64>,
}

impl Default for LLMStats {
    fn default() -> Self {
        Self {
            total_requests: 0,
            successful_requests: 0,
            failed_requests: 0,
            avg_latency_ms: 0.0,
            min_latency_ms: u64::MAX,
            max_latency_ms: 0,
            p50_latency_ms: 0,
            p95_latency_ms: 0,
            p99_latency_ms: 0,
            total_input_tokens: 0,
            total_output_tokens: 0,
            total_tokens: 0,
            total_cost: 0.0,
            error_rate: 0.0,
            errors_by_type: HashMap::new(),
        }
    }
}

/// LLM æ€§èƒ½ç›‘æ§å™¨
pub struct LLMMonitor {
    /// æŒ‡æ ‡å†å²è®°å½•
    metrics_history: Arc<Mutex<Vec<LLMMetrics>>>,
    /// æ˜¯å¦å¯ç”¨
    enabled: bool,
    /// æœ€å¤§å†å²è®°å½•æ•°
    max_history_size: usize,
}

impl LLMMonitor {
    /// åˆ›å»ºæ–°çš„ç›‘æ§å™¨
    pub fn new(enabled: bool, max_history_size: usize) -> Self {
        Self {
            metrics_history: Arc::new(Mutex::new(Vec::new())),
            enabled,
            max_history_size,
        }
    }

    /// ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºç›‘æ§å™¨
    pub fn default() -> Self {
        Self::new(true, 1000)
    }

    /// ç¦ç”¨ç›‘æ§
    pub fn disabled() -> Self {
        Self::new(false, 0)
    }

    /// è®°å½•æŒ‡æ ‡
    pub fn record(&self, metrics: LLMMetrics) {
        if !self.enabled {
            return;
        }

        debug!(
            "ğŸ“Š LLM Metrics: provider={}, model={}, latency={}ms, tokens={}, success={}",
            metrics.provider, metrics.model, metrics.latency_ms, metrics.total_tokens, metrics.success
        );

        let mut history = self.metrics_history.lock().unwrap();
        history.push(metrics);

        // é™åˆ¶å†å²è®°å½•å¤§å°
        if history.len() > self.max_history_size {
            history.remove(0);
        }
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> LLMStats {
        if !self.enabled {
            return LLMStats::default();
        }

        let history = self.metrics_history.lock().unwrap();
        if history.is_empty() {
            return LLMStats::default();
        }

        let total_requests = history.len() as u64;
        let successful_requests = history.iter().filter(|m| m.success).count() as u64;
        let failed_requests = total_requests - successful_requests;

        // è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
        let mut latencies: Vec<u64> = history.iter().map(|m| m.latency_ms).collect();
        latencies.sort_unstable();

        let sum_latency: u64 = latencies.iter().sum();
        let avg_latency_ms = sum_latency as f64 / total_requests as f64;
        let min_latency_ms = *latencies.first().unwrap_or(&0);
        let max_latency_ms = *latencies.last().unwrap_or(&0);

        let p50_index = (total_requests as f64 * 0.50) as usize;
        let p95_index = (total_requests as f64 * 0.95) as usize;
        let p99_index = (total_requests as f64 * 0.99) as usize;

        let p50_latency_ms = latencies.get(p50_index).copied().unwrap_or(0);
        let p95_latency_ms = latencies.get(p95_index).copied().unwrap_or(0);
        let p99_latency_ms = latencies.get(p99_index).copied().unwrap_or(0);

        // è®¡ç®— token ç»Ÿè®¡
        let total_input_tokens: u64 = history.iter().map(|m| m.input_tokens as u64).sum();
        let total_output_tokens: u64 = history.iter().map(|m| m.output_tokens as u64).sum();
        let total_tokens = total_input_tokens + total_output_tokens;

        // è®¡ç®—æˆæœ¬
        let total_cost: f64 = history.iter().map(|m| m.estimated_cost()).sum();

        // è®¡ç®—é”™è¯¯ç‡
        let error_rate = failed_requests as f64 / total_requests as f64;

        // æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
        let mut errors_by_type: HashMap<String, u64> = HashMap::new();
        for metric in history.iter() {
            if let Some(error_type) = &metric.error_type {
                *errors_by_type.entry(error_type.clone()).or_insert(0) += 1;
            }
        }

        LLMStats {
            total_requests,
            successful_requests,
            failed_requests,
            avg_latency_ms,
            min_latency_ms,
            max_latency_ms,
            p50_latency_ms,
            p95_latency_ms,
            p99_latency_ms,
            total_input_tokens,
            total_output_tokens,
            total_tokens,
            total_cost,
            error_rate,
            errors_by_type,
        }
    }

    /// æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    pub fn print_stats(&self) {
        let stats = self.get_stats();
        info!("ğŸ“Š LLM Performance Statistics:");
        info!("  Total Requests: {}", stats.total_requests);
        info!("  Successful: {} ({:.2}%)", stats.successful_requests, (stats.successful_requests as f64 / stats.total_requests as f64) * 100.0);
        info!("  Failed: {} ({:.2}%)", stats.failed_requests, stats.error_rate * 100.0);
        info!("  Latency: avg={:.2}ms, min={}ms, max={}ms", stats.avg_latency_ms, stats.min_latency_ms, stats.max_latency_ms);
        info!("  Latency Percentiles: P50={}ms, P95={}ms, P99={}ms", stats.p50_latency_ms, stats.p95_latency_ms, stats.p99_latency_ms);
        info!("  Tokens: input={}, output={}, total={}", stats.total_input_tokens, stats.total_output_tokens, stats.total_tokens);
        info!("  Estimated Cost: ${:.4}", stats.total_cost);
        
        if !stats.errors_by_type.is_empty() {
            info!("  Errors by Type:");
            for (error_type, count) in &stats.errors_by_type {
                info!("    {}: {}", error_type, count);
            }
        }
    }

    /// é‡ç½®ç»Ÿè®¡ä¿¡æ¯
    pub fn reset(&self) {
        if !self.enabled {
            return;
        }

        let mut history = self.metrics_history.lock().unwrap();
        history.clear();
        info!("ğŸ”„ LLM metrics reset");
    }

    /// è·å–å†å²è®°å½•æ•°é‡
    pub fn history_size(&self) -> usize {
        if !self.enabled {
            return 0;
        }

        let history = self.metrics_history.lock().unwrap();
        history.len()
    }
}

impl Clone for LLMMonitor {
    fn clone(&self) -> Self {
        Self {
            metrics_history: Arc::clone(&self.metrics_history),
            enabled: self.enabled,
            max_history_size: self.max_history_size,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_metrics_creation() {
        let metrics = LLMMetrics::new("openai".to_string(), "gpt-4".to_string())
            .with_latency(Duration::from_millis(500))
            .with_tokens(100, 200)
            .with_success(true);

        assert_eq!(metrics.provider, "openai");
        assert_eq!(metrics.model, "gpt-4");
        assert_eq!(metrics.latency_ms, 500);
        assert_eq!(metrics.input_tokens, 100);
        assert_eq!(metrics.output_tokens, 200);
        assert_eq!(metrics.total_tokens, 300);
        assert!(metrics.success);
    }

    #[test]
    fn test_cost_estimation() {
        let metrics = LLMMetrics::new("openai".to_string(), "gpt-4".to_string())
            .with_tokens(1000, 2000);

        let cost = metrics.estimated_cost();
        assert!(cost > 0.0);
    }

    #[test]
    fn test_monitor() {
        let monitor = LLMMonitor::default();

        let metrics1 = LLMMetrics::new("openai".to_string(), "gpt-4".to_string())
            .with_latency(Duration::from_millis(500))
            .with_tokens(100, 200)
            .with_success(true);

        let metrics2 = LLMMetrics::new("openai".to_string(), "gpt-4".to_string())
            .with_latency(Duration::from_millis(600))
            .with_tokens(150, 250)
            .with_success(false)
            .with_error("timeout".to_string());

        monitor.record(metrics1);
        monitor.record(metrics2);

        let stats = monitor.get_stats();
        assert_eq!(stats.total_requests, 2);
        assert_eq!(stats.successful_requests, 1);
        assert_eq!(stats.failed_requests, 1);
        assert_eq!(stats.error_rate, 0.5);
    }
}

