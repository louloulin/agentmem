//! Async batch processing for improved performance
//!
//! This module provides efficient batch processing capabilities for memory operations,
//! reducing overhead and improving throughput.

use agent_mem_traits::{AgentMemError, Result};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::VecDeque;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, RwLock, Semaphore};
use tokio::time::{interval, timeout};
use tracing::{debug, error, info, warn};

/// Batch processing configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchConfig {
    /// Maximum batch size
    pub max_batch_size: usize,
    /// Maximum wait time before processing incomplete batch
    pub max_wait_time_ms: u64,
    /// Number of concurrent batch processors
    pub concurrency: usize,
    /// Buffer size for pending items
    pub buffer_size: usize,
    /// Enable batch compression
    pub enable_compression: bool,
    /// Retry configuration
    pub retry_attempts: u32,
    pub retry_delay_ms: u64,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 100,
            max_wait_time_ms: 1000,
            concurrency: 4,
            buffer_size: 10000,
            enable_compression: false,
            retry_attempts: 3,
            retry_delay_ms: 100,
        }
    }
}

/// Batch processing result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchResult {
    pub processed_items: u64,
    pub failed_items: u64,
    pub total_batches: u64,
    pub average_batch_size: f64,
    pub average_processing_time_ms: f64,
    pub throughput_items_per_second: f64,
}

impl Default for BatchResult {
    fn default() -> Self {
        Self {
            processed_items: 0,
            failed_items: 0,
            total_batches: 0,
            average_batch_size: 0.0,
            average_processing_time_ms: 0.0,
            throughput_items_per_second: 0.0,
        }
    }
}

/// Batch item trait
#[async_trait]
pub trait BatchItem: Send + Sync + 'static {
    type Output: Send + Sync;
    type Error: Send + Sync + std::error::Error;

    /// Process a single item
    async fn process(&self) -> std::result::Result<Self::Output, Self::Error>;

    /// Get item size for batching decisions
    fn size(&self) -> usize {
        1
    }

    /// Get item priority (higher = more priority)
    fn priority(&self) -> u8 {
        0
    }
}

/// Batch processor
pub struct BatchProcessor {
    config: BatchConfig,
    sender: mpsc::UnboundedSender<BatchRequest>,
    stats: Arc<RwLock<BatchResult>>,
    semaphore: Arc<Semaphore>,
}

/// Internal batch request
struct BatchRequest {
    item: Box<dyn BatchItem<Output = Vec<u8>, Error = AgentMemError>>,
    response_tx: tokio::sync::oneshot::Sender<Result<Vec<u8>>>,
}

impl BatchProcessor {
    /// Create a new batch processor
    pub async fn new(config: BatchConfig) -> Result<Self> {
        let (sender, receiver) = mpsc::unbounded_channel();
        let stats = Arc::new(RwLock::new(BatchResult::default()));
        let semaphore = Arc::new(Semaphore::new(config.concurrency));

        let processor = Self {
            config: config.clone(),
            sender,
            stats: Arc::clone(&stats),
            semaphore: Arc::clone(&semaphore),
        };

        // Start batch processing workers
        let receiver = Arc::new(tokio::sync::Mutex::new(receiver));
        for worker_id in 0..config.concurrency {
            let worker = BatchWorker::new(
                worker_id,
                config.clone(),
                Arc::clone(&receiver),
                Arc::clone(&stats),
                Arc::clone(&semaphore),
            );
            tokio::spawn(async move {
                worker.run().await;
            });
        }

        info!(
            "Batch processor started with {} workers",
            config.concurrency
        );
        Ok(processor)
    }

    /// Submit an item for batch processing
    pub async fn submit<T>(&self, item: T) -> Result<T::Output>
    where
        T: BatchItem,
    {
        let (response_tx, response_rx) = tokio::sync::oneshot::channel();

        // Convert to boxed trait object
        let boxed_item = Box::new(GenericBatchItem::new(item));

        let request = BatchRequest {
            item: boxed_item,
            response_tx,
        };

        self.sender
            .send(request)
            .map_err(|_| AgentMemError::memory_error("Batch processor channel closed"))?;

        let result = response_rx
            .await
            .map_err(|_| AgentMemError::memory_error("Batch processing response lost"))?;

        // Convert back to original type
        match result {
            Ok(data) => {
                // This is a simplified conversion - in practice you'd need proper serialization
                Ok(unsafe { std::mem::transmute_copy(&data) })
            }
            Err(e) => Err(e),
        }
    }

    /// Get processing statistics
    pub async fn get_stats(&self) -> Result<BatchResult> {
        Ok(self.stats.read().await.clone())
    }

    /// Shutdown the batch processor
    pub async fn shutdown(&self) -> Result<()> {
        // Close the sender to signal shutdown
        // Note: We can't drop the sender here as it's behind a shared reference
        // The sender will be dropped when the BatchProcessor is dropped
        info!("Batch processor shutdown initiated");
        Ok(())
    }
}

/// Generic wrapper for batch items
struct GenericBatchItem<T: BatchItem> {
    inner: T,
}

impl<T: BatchItem> GenericBatchItem<T> {
    fn new(item: T) -> Self {
        Self { inner: item }
    }
}

#[async_trait]
impl<T: BatchItem> BatchItem for GenericBatchItem<T> {
    type Output = Vec<u8>;
    type Error = AgentMemError;

    async fn process(&self) -> std::result::Result<Self::Output, Self::Error> {
        match self.inner.process().await {
            Ok(output) => {
                // Serialize the output - simplified for demo
                Ok(format!("{:?}", std::any::type_name::<T::Output>()).into_bytes())
            }
            Err(e) => Err(AgentMemError::memory_error(&e.to_string())),
        }
    }

    fn size(&self) -> usize {
        self.inner.size()
    }

    fn priority(&self) -> u8 {
        self.inner.priority()
    }
}

/// Batch processing worker
struct BatchWorker {
    id: usize,
    config: BatchConfig,
    receiver: Arc<tokio::sync::Mutex<mpsc::UnboundedReceiver<BatchRequest>>>,
    stats: Arc<RwLock<BatchResult>>,
    semaphore: Arc<Semaphore>,
    batch_buffer: VecDeque<BatchRequest>,
    last_flush: Instant,
}

impl BatchWorker {
    fn new(
        id: usize,
        config: BatchConfig,
        receiver: Arc<tokio::sync::Mutex<mpsc::UnboundedReceiver<BatchRequest>>>,
        stats: Arc<RwLock<BatchResult>>,
        semaphore: Arc<Semaphore>,
    ) -> Self {
        Self {
            id,
            config,
            receiver,
            stats,
            semaphore,
            batch_buffer: VecDeque::new(),
            last_flush: Instant::now(),
        }
    }

    async fn run(mut self) {
        debug!("Batch worker {} started", self.id);

        let mut flush_interval = interval(Duration::from_millis(self.config.max_wait_time_ms));

        loop {
            tokio::select! {
                // Receive new items
                request = async {
                    let mut receiver = self.receiver.lock().await;
                    receiver.recv().await
                } => {
                    match request {
                        Some(req) => {
                            self.batch_buffer.push_back(req);

                            // Check if we should flush
                            if self.should_flush() {
                                self.flush_batch().await;
                            }
                        }
                        None => {
                            // Channel closed, flush remaining and exit
                            if !self.batch_buffer.is_empty() {
                                self.flush_batch().await;
                            }
                            break;
                        }
                    }
                }

                // Periodic flush
                _ = flush_interval.tick() => {
                    if !self.batch_buffer.is_empty() && self.should_flush_by_time() {
                        self.flush_batch().await;
                    }
                }
            }
        }

        debug!("Batch worker {} stopped", self.id);
    }

    fn should_flush(&self) -> bool {
        self.batch_buffer.len() >= self.config.max_batch_size || self.should_flush_by_time()
    }

    fn should_flush_by_time(&self) -> bool {
        self.last_flush.elapsed() >= Duration::from_millis(self.config.max_wait_time_ms)
    }

    async fn flush_batch(&mut self) {
        if self.batch_buffer.is_empty() {
            return;
        }

        let _permit = self.semaphore.acquire().await.unwrap();
        let batch_size = self.batch_buffer.len();
        let start_time = Instant::now();

        debug!(
            "Worker {} processing batch of {} items",
            self.id, batch_size
        );

        // Process all items in the batch
        let mut processed = 0;
        let mut failed = 0;

        while let Some(request) = self.batch_buffer.pop_front() {
            let result = self.process_item(request.item).await;

            match result {
                Ok(output) => {
                    processed += 1;
                    let _ = request.response_tx.send(Ok(output));
                }
                Err(e) => {
                    failed += 1;
                    let _ = request.response_tx.send(Err(e));
                }
            }
        }

        let processing_time = start_time.elapsed();
        self.last_flush = Instant::now();

        // Update statistics
        self.update_stats(batch_size, processed, failed, processing_time)
            .await;

        debug!(
            "Worker {} completed batch: {} processed, {} failed, took {:?}",
            self.id, processed, failed, processing_time
        );
    }

    async fn process_item(
        &self,
        item: Box<dyn BatchItem<Output = Vec<u8>, Error = AgentMemError>>,
    ) -> Result<Vec<u8>> {
        let mut attempts = 0;

        loop {
            match item.process().await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    attempts += 1;
                    if attempts >= self.config.retry_attempts {
                        return Err(AgentMemError::memory_error(&format!(
                            "Failed after {} attempts: {}",
                            attempts, e
                        )));
                    }

                    tokio::time::sleep(Duration::from_millis(self.config.retry_delay_ms)).await;
                }
            }
        }
    }

    async fn update_stats(
        &self,
        batch_size: usize,
        processed: u64,
        failed: u64,
        duration: Duration,
    ) {
        let mut stats = self.stats.write().await;

        stats.processed_items += processed;
        stats.failed_items += failed;
        stats.total_batches += 1;

        // Update averages
        let total_items = stats.processed_items + stats.failed_items;
        if total_items > 0 {
            stats.average_batch_size = total_items as f64 / stats.total_batches as f64;
        }

        let duration_ms = duration.as_millis() as f64;
        if stats.total_batches > 0 {
            stats.average_processing_time_ms =
                (stats.average_processing_time_ms * (stats.total_batches - 1) as f64 + duration_ms)
                    / stats.total_batches as f64;
        }

        // Calculate throughput (items per second)
        if duration_ms > 0.0 {
            stats.throughput_items_per_second = batch_size as f64 / (duration_ms / 1000.0);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    struct TestBatchItem {
        data: String,
        should_fail: bool,
    }

    #[async_trait]
    impl BatchItem for TestBatchItem {
        type Output = String;
        type Error = AgentMemError;

        async fn process(&self) -> std::result::Result<Self::Output, Self::Error> {
            if self.should_fail {
                Err(AgentMemError::memory_error("Test failure"))
            } else {
                Ok(format!("processed: {}", self.data))
            }
        }
    }

    #[tokio::test]
    async fn test_batch_processor_creation() {
        let config = BatchConfig::default();
        let processor = BatchProcessor::new(config).await;
        assert!(processor.is_ok());
    }

    #[tokio::test]
    async fn test_batch_processing() {
        let config = BatchConfig {
            max_batch_size: 2,
            max_wait_time_ms: 100,
            ..Default::default()
        };

        let processor = BatchProcessor::new(config).await.unwrap();

        let item = TestBatchItem {
            data: "test".to_string(),
            should_fail: false,
        };

        let result = processor.submit(item).await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_batch_stats() {
        let config = BatchConfig::default();
        let processor = BatchProcessor::new(config).await.unwrap();

        let stats = processor.get_stats().await.unwrap();
        assert_eq!(stats.processed_items, 0);
        assert_eq!(stats.failed_items, 0);
    }
}
